{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info\n",
    "\n",
    "This notebook contains several demos for usage of the ghostwheel inference server. [See the readme](readme.md) for more info.\n",
    "\n",
    "Note that **you'll need to run this notebook on the internal network (Imperial-WPA)** as the server hosting ghostwheel is not exposed externally.\n",
    "\n",
    "### Available LLMs\n",
    "\n",
    "[The ghostwheel API docs](https://ese-timewarp.ese.ic.ac.uk) contain an up-to-date list of valid identifiers for LLMs you can call through ghostwheel. The docs are regenerated with any change to the backend application, so this list is kept current when any new models are deployed. In addition to the Ollama calls mentioned above, we also provide a `api/list_models` endpoint, should you want to programatically determine which LLMs are available to you.\n",
    "\n",
    "For more information describing parameters for the completion endpoints and response specifications, [check out the Ollama API docs on GitHub](https://github.com/ollama/ollama/blob/main/docs/api.md).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common setup\n",
    "\n",
    "We define two global constants here that we'll use throughout the notebook, namely, for the ghostwheel base URL and for our API key. In practice you should store your key as an environment variable (or otherwise outside of your codebase) and load it dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base URL and API key for ghostwheel\n",
    "GHOSTWHEEL_BASE_URL = \"https://ese-timewarp.ese.ic.ac.uk\"\n",
    "GHOSTWHEEL_API_KEY = \"{your api key here}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ghostwheel as a REST API\n",
    "\n",
    "The most direct (and arguably least convenient) way to use ghostwheel is with REST API calls. This has the advantage of working with practically any language and on any platform. Below is an example of calling the `api/generate` endpoint from Python, though if you're using Python for your application, we'll see in a moment that there are a few alternatives for communication with ghostwheel that are far more convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We'll install the requests package to make our API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the requests library\n",
    "%pip install -q requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': [128006, 882, 128007, 271],\n",
      " 'created_at': '2024-06-12T12:55:03.111223323Z',\n",
      " 'done': True,\n",
      " 'done_reason': 'stop',\n",
      " 'eval_count': 79,\n",
      " 'eval_duration': 4690938000,\n",
      " 'load_duration': 6250379,\n",
      " 'model': 'llama3:70b-instruct-q8_0',\n",
      " 'prompt_eval_duration': 63489000,\n",
      " 'response': 'Here is a brief history of the United States:\\n'\n",
      "             '\\n'\n",
      "             'The US was founded in 1776 by European colonists who declared '\n",
      "             'independence from Britain. The young nation expanded westward, '\n",
      "             'survived a civil war (1861-1865), and emerged as a global '\n",
      "             'superpower through industrialization, World War I and II, and '\n",
      "             'the Cold War, shaping its diverse culture and economy along the '\n",
      "             'way.',\n",
      " 'total_duration': 4804171071}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "# Generates a response from the inference server\n",
    "def generate(model: str, prompt: str) -> dict:\n",
    "    url = f\"{GHOSTWHEEL_BASE_URL}/api/generate\"\n",
    "    res = requests.post(\n",
    "        url,\n",
    "        json={\n",
    "            'model': model,\n",
    "            'prompt': prompt,\n",
    "            'stream': False,\n",
    "        },\n",
    "        headers={\n",
    "            'Content-Type': \"application/json\", # Must specify JSON content type\n",
    "            'X-API-Key': GHOSTWHEEL_API_KEY,\n",
    "        },\n",
    "    )\n",
    "    res.raise_for_status()\n",
    "    return res.json()\n",
    "\n",
    "\n",
    "# Get a response from the model\n",
    "res = generate(\n",
    "    'llama3:70b-instruct-q8_0',\n",
    "    \"Tell me about the history of the United States of America in 50 words or less.\",\n",
    ")\n",
    "\n",
    "res['context'] = res['context'][:4]  # Truncate the context so output isn't long as heck\n",
    "pprint(res) # Pretty print the response in JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ghostwheel with the Python ollama package\n",
    "\n",
    "Direct API call are portable, sure, but why reinvent the wheel? Ollama provides a convenient API client for Python [in the form of the `ollama` package](https://github.com/ollama/ollama-python), which we can trivially use with ghostwheel as well. Naturally, the endpoints ghostwheel doesn't implement (i.e. for administration of served models) will not work, so you'll be restricted to `api/generate`, `api/chat` and `api/tags`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the ollama package\n",
    "%pip install -q ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic completions\n",
    "\n",
    "To use the ollama package with ghostwheel, we just need to set its host parameter and provide our API key in the client's default headers. Note that we *cannot* directly call `ollama.generate()` or the other functions directly from the module, since it uses a default client that's not configured to use ghostwheel. Instead, we initialize an `ollama.Client` instance ourselves and configure it as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisherman's Wharf is a popular tourist destination in San Francisco, California. This bustling waterfront district offers stunning views of the Bay Bridge, Alcatraz Island, and the sea lions at Pier 39. Visitors can sample fresh seafood, browse souvenir shops, and take a stroll along the pier. Street performers and live music add to the lively atmosphere. Take a ferry to Alcatraz or enjoy a sunset cruise from one of the many piers. With its rich maritime history and vibrant energy, Fisherman's Wharf is a must-visit spot in San Francisco.\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client as OllamaClient\n",
    "\n",
    "# Initialize the Ollama client with ghostwheel and provide our API key\n",
    "client = OllamaClient(\n",
    "    host=GHOSTWHEEL_BASE_URL,\n",
    "    headers={\n",
    "        'X-API-Key': GHOSTWHEEL_API_KEY,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "# Generate a response from the model\n",
    "res = client.generate(\n",
    "    'llama3:70b-instruct-q8_0',\n",
    "    \"Tell me about Fisherman's Wharf in San Francisco in at most 100 words.\",\n",
    ")\n",
    "print(res['response']) # Print the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': '2024-06-12T12:53:49.281011657Z',\n",
      " 'done': True,\n",
      " 'done_reason': 'stop',\n",
      " 'eval_count': 8,\n",
      " 'eval_duration': 241640000,\n",
      " 'load_duration': 10414176,\n",
      " 'message': {'content': ' The capital of France is Paris.',\n",
      "             'role': 'assistant'},\n",
      " 'model': 'mistral:7b-instruct-v0.3-q8_0',\n",
      " 'prompt_eval_count': 2,\n",
      " 'prompt_eval_duration': 42765000,\n",
      " 'total_duration': 426845674}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Generate a chat response\n",
    "res = client.chat(\n",
    "    'mistral:7b-instruct-v0.3-q8_0',\n",
    "    [\n",
    "        {\n",
    "            'role': \"user\",\n",
    "            'content': \"What is the capital of France?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Pretty print the whole response object\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response streaming\n",
    "\n",
    "We can stream responses easily with the ollama package as well, with the caveat that it doesn't provide any asynchronous methods—use Langchain if you need to stream responses asynchronously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris, the City of Light, is renowned for its romantic ambiance, iconic landmarks, and rich artistic heritage. Known as a global center for art, fashion, gastronomy, and culture, it's home to masterpieces like the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and more. The Seine River winds through the city, offering picturesque views of its historical architecture. Parisian cuisine, from croissants to haute couture dining, delights visitors worldwide. Its vibrant energy, captured in the twirl of dancers at the Moulin Rouge, encapsulates the essence of a timeless, enchanting city."
     ]
    }
   ],
   "source": [
    "# Get a response stream from the model\n",
    "stream = client.generate(\n",
    "    'mistral:7b-instruct-v0.3-q8_0',\n",
    "    \"Tell me about Paris in 100 words or less.\",\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "# Iterate over the response stream\n",
    "for res in stream:\n",
    "    print(res['response'], end='')  # Print incoming response tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ghostwheel with Langchain\n",
    "\n",
    "Langchain has built-in support for using Ollama models for text and chat completion; we can take advantage of these integrations to use ghostwheel as its backend. All that's required is to point an `Ollama` instance (or `ChatOllama`, `OllamaEmbeddings` etc. as needed) at ghostwheel with the `base_url` parameter, and provide our API key in the request headers (Langchain allows us to define headers for its API calls when constructing an instance of these classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "To get ghosthweel running behind langchain, we only need the `langchain` and `langchain-community` packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q \\\n",
    "    langchain \\\n",
    "    langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is a simple function that does that:\n",
       "```\n",
       "def sum_list(numbers):\n",
       "    return sum(numbers)\n",
       "```\n",
       "Explanation:\n",
       "\n",
       "* The `sum` function is a built-in Python function that takes an iterable (such as a list) as an argument and returns the sum of all its elements.\n",
       "* In this case, of a list of integers, we simply pass the list to the `sum` function and it will return the sum of all the integers in the list.\n",
       "\n",
       "Example usage:\n",
       "```\n",
       "numbers = [1, 2, 3, 4, 5]\n",
       "result = sum_list(numbers)\n",
       "print(result)  # Output: 15\n",
       "```\n",
       "Note that this function assumes that the input list only contains integers. If the list can contain other types of elements, you may want to add error handling or type checking to ensure that the function behaves correctly."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "llm = Ollama(\n",
    "    model='llama3:70b-instruct-q8_0', # Any of the available models listed in the API docs\n",
    "    base_url=GHOSTWHEEL_BASE_URL,\n",
    "    headers={\n",
    "        'X-API-Key': GHOSTWHEEL_API_KEY,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Get a response from the model\n",
    "prompt = \"Write a function that takes a list of integers and returns the sum of all the integers in the list.\"\n",
    "res = llm.invoke(prompt)\n",
    "\n",
    "# Print the response\n",
    "display(Markdown(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response streaming\n",
    "We can trivially stream the response and output tokens as they are received from the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a brief 100-word history of the United States:\n",
      "\n",
      "The United States declared independence from Britain in 1776 and fought a Revolutionary War to gain freedom. The young nation expanded westward, acquiring land through the Louisiana Purchase and Mexican-American War. Civil War (1861-1865) ended slavery, but racial tensions persisted. Industrialization and immigration shaped the country in the late 1800s and early 1900s. The US emerged as a world power after World Wars I and II, playing a key role in shaping global politics during the Cold War era. Social movements of the 1960s, including Civil Rights and Feminist movements, continued to shape American society."
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me about the history of the United States in 100 words.\"\n",
    "\n",
    "# Asynchronously stream tokens from the model\n",
    "async for token in llm.astream(prompt):\n",
    "    # Print each token as it's received (without the newline delimiter)\n",
    "    print(token, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "We can use ghostwheel for embeddings as well. First we'll install some requirements for document loading (`pypdf`) and for RAG further down (`langchain-chroma` to use as a vector database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install requirements for document loading & RAG\n",
    "%pip install -q \\\n",
    "    langchain-chroma \\\n",
    "    pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded document (22 chunks):\n",
      "\n",
      "Future-of-chipmaking\n",
      "TWO YEARS shy of its 60th birthday , Moore’ s law has become a bit like Schrödinger ’s hypothetical cat—at once dead and alive. In\n",
      "1965 Gordon Moore, one of the co-founders of Intel, observed that the number of transistors—a type of electronic component—\n",
      "that could be crammed onto a microchip was doubling every 12 months, a figure he later revised to every two years.\n",
      "\n",
      "...\n",
      "\n",
      "At some point, the day will arrive when no amount of clever technology can shrink transistors still further (it is hard to see, for\n",
      "instance, how one could be built with less than an atom’ s worth of stuf f). As Moore himself warned in 2003, “no exponential is for\n",
      "ever.” But, he told the assembled engineers, “your job is delaying for ever”. Chipmakers have done an admirable job of that in the\n",
      "two decades since he spoke. And they have at least sketched out a path for the next two decades, too.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the embeddings model\n",
    "emb = OllamaEmbeddings(\n",
    "    model='nomic-embed-text:137m-v1.5-fp16',\n",
    "    base_url=GHOSTWHEEL_BASE_URL,\n",
    "    headers={\n",
    "        'X-API-Key': GHOSTWHEEL_API_KEY,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Load a PDF document and split into chunks\n",
    "# See https://github.com/ollama/ollama/blob/main/examples/langchain-python-rag-document/main.py\n",
    "loader = PyPDFLoader(\"./assets/future_of_chipmaking.pdf\")\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "docs = loader.load()\n",
    "doc_chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Print the first and last chunks\n",
    "print(f\"Loaded document ({len(doc_chunks)} chunks):\\n\")\n",
    "print(f\"{doc_chunks[0].page_content}\\n\\n...\\n\\n{doc_chunks[-1].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Researchers are looking into abandoning silicon because at tiny sizes (less than four nanometers), current leakage becomes much worse due to silicon's surface roughness, which hinders electron movement and reduces the transistor's ability to switch on and off properly.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.chat_models.ollama import ChatOllama\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "# Load the document chunks into a vector database\n",
    "db = Chroma.from_documents(documents=doc_chunks, embedding=emb)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Set up a chain for RAG question/answers\n",
    "sys_prompt = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following \\\n",
    "pieces of retrieved contextto answer the question. If you don't know \\\n",
    "the answer, just say that you don't know. Use three sentences \\\n",
    "maximum and keep the answer concise.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', sys_prompt),\n",
    "    ('human', \"{input}\")\n",
    "])\n",
    "\n",
    "# Set up a chat model\n",
    "rag_model = ChatOllama(\n",
    "    model='llama3:70b-instruct-q8_0',\n",
    "    base_url=GHOSTWHEEL_BASE_URL,\n",
    "    headers={\n",
    "        'X-API-Key': GHOSTWHEEL_API_KEY,\n",
    "    },\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Create the RAG chain\n",
    "docs_chain = create_stuff_documents_chain(rag_model, rag_prompt)\n",
    "chain = create_retrieval_chain(retriever, docs_chain)\n",
    "\n",
    "# Retrieves an answer\n",
    "def query(question: str) -> dict:\n",
    "    res = chain.invoke({\n",
    "        'input': question,\n",
    "    })\n",
    "    return res\n",
    "\n",
    "# Ask a question based on the document\n",
    "res = query(\"Why are researchers looking into abandoning silicon?\")\n",
    "print(res['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
